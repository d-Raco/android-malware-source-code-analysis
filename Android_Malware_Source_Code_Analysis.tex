\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{booktabs}
\usepackage{lastpage}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage[hidelinks]{hyperref}
\usepackage{caption}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\pagestyle{fancy}
\fancyhf{}
\rfoot{Page \thepage \hspace{1pt} of \pageref{LastPage}}

\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=blue,
	citecolor = blue,      
	urlcolor=blue,
}

\definecolor{azulUC3M}{RGB}{0,0,102}
\definecolor{gray97}{gray}{.97}
\definecolor{gray75}{gray}{.75}
\definecolor{gray45}{gray}{.45}

\begin{document}
	
\begin{titlepage}
	\begin{sffamily}
		\color{azulUC3M}
		\begin{center}
			\begin{figure} %incluimos el logotipo de la Universidad
				\makebox[\textwidth][c]{\includegraphics[width=16cm]{imagenes/Portada_Logo.png}}
			\end{figure}
			\vspace{1cm}
			\begin{Large}
				Master Degree in Cybersecurity\\			
				2021-2022\\
				\vspace{1cm}		
				\textsl{Master Thesis}
				\bigskip
				
			\end{Large}
			{\Huge ``Android Malware Source Code Analysis''}\\
			\vspace*{0.5cm}
			\rule{10.5cm}{0.1mm}\\
			\vspace*{0.9cm}
			{\LARGE Ramón Costales de Ledesma}\\ 
			\vspace*{1cm}
			\begin{Large}
				Juan Tapiador\\
				Madrid, Spain, September 19th. 2022\\
			\end{Large}
		\end{center}
		
		\vfill
		\color{black}
		\begin{footnotesize}
			\noindent\fbox{
				\begin{minipage}{\textwidth}
					\textbf{AVOID PLAGIARISM}\\
					The University uses the \textbf{Turnitin Feedback Studio} program within the Aula Global for the delivery of student work. This program compares the originality of the work delivered by each student with millions of electronic resources and detects those parts of the text that are copied and pasted. Plagiarizing in a TFM is considered a \textbf{Serious Misconduct}, and may result in permanent expulsion from the University.
				\end{minipage}	
			}
			\vspace*{.5cm}\\	
			\noindent\includegraphics[width=4.2cm]{imagenes/creativecommons.png}\\
			\emph{[Include this code in case you want your Master Thesis published in Open Access University Repository]}\\
			This work is licensed under Creative Commons \textbf{Attribution – Non Commercial – Non Derivatives}
			
		\end{footnotesize}
	\end{sffamily}
\end{titlepage}

\newpage %página en blanco o de cortesía
\thispagestyle{empty}
\mbox{}

\title{Android Malware Source Code Analysis\\
{\footnotesize Master's Thesis in Cybersecurity, 2021/2022}
}

\author{
\IEEEauthorblockN{Ramón Costales}
\IEEEauthorblockA{\textit{Student} \\ 
\textit{Department of Computer Science,} \\
\textit{Universidad Carlos III de Madrid,}\\
Madrid, Spain}
\and
\IEEEauthorblockN{Juan Tapiador}
\IEEEauthorblockA{\textit{Advisor} \\ 
\textit{Department of Computer Science,} \\
\textit{Universidad Carlos III de Madrid,}\\
Madrid, Spain}
\and
}

\maketitle

\begin{abstract}
Since the emergence of malware in the 1970s, these malicious programs have steadily increased in number and sophistication. The increasing profits generated by the use of malware have led to a growing demand, turning malware into a commodity of the underground economy. In this thesis, we analyze the evolution of Android malware from 2012 to date from a software engineering perspective. We analyze the source code of 97 samples from 83 unique families and obtain measures of their size, code quality, and estimates of the development costs (effort, time, and number of people). Our results suggest a linear increment per year in aspects such as number of malware samples and size, as well as a rapid increase in development cost. In terms of complexity and maintainability, we observe a low score compared to malware on other operating systems. Overall, our results are not conclusive enough to support claims about the increasing complexity of Android malware and its production progressively becoming an industry. This could be due to the fact that the Android malware industry is still young, as the operating system itself was launched just over ten years ago, or that there has been little change in the computer industry since the release of Android compared to the progress made in previous decades.
\end{abstract}

\begin{IEEEkeywords}
Android; malware; source code analysis; software metrics
\end{IEEEkeywords}

\section{Introduction}

Every day, the mobile landscape grows in size. Recently, Google announced that Android had surpassed \textit{three billion} users \cite{b1}, securing the largest mobile market share for another year. Every year, the number of mobile users increases, causing malware to follow that trend. But malware is not only growing in the mobile landscape. As AVTEST's Malware Statistics clearly show \cite{b2}, 2021 saw an extreme increase in new malware discovered, exceeding \textit{150,000,000} new samples that year. Android is not far behind, as \textit{3,000,000} new samples were discovered for this operating system.

A 2021 Report by Malware Bytes \cite{b3} confirmed that malware as a business is a growing trend, taking up more real estate in the cyber-threat landscape, making malware development more profitable. On Android, most malware developers fund their operations by generating ad revenue, while others deploy ransomware or large botnets for profit. It is also mentioned that stalkerware and spyware-type applications experienced a detection increase of \textit{1,677\%} in 2021, which is consistent with the types of malware present in our dataset. Some malware developers even leveraged the current global situation to use the COVID-19 pandemic as a cover to deploy malware and infect unsuspected victims; two samples in our dataset are ransomware disguised behind this facade.

As the number and profitability of Android malware increases, so does the sophistication and impact of attacks. In this thesis, we present a study of the evolution of Android malware from a software engineering approach. Our analysis is based on a dataset collected by the authors over several months and composed of the source code of 97 Android malware samples ranging from 2012 to 2022. Our dataset includes, among others, RATs, Trojan-Bankers, Keyloggers, Ransomware, Spyware, and Lockers. This is the largest Android malware source code dataset presented in the literature. We perform several analyses on this dataset. First, we review the most prevalent malware types and the most common permissions and capabilities used by the collected samples, as well as their antivirus detection rate. We also measure the evolution of malware development as a function of size, cost and quality. 

Size dimensions are measured with several metrics, mainly the number of source files, the number of source lines of code (SLOCs), the number of functions, and the number of different programming languages used. Development cost is calculated with three estimates: effort, development time, and team size. Finally, code quality measures are computed using the cyclomatic complexity, the maintainability index, and the density of comments present in the code. 

We then compare the results obtained with other similar works \cite{b4}, \cite{b5} that performed the same measurements, but without specializing in a single platform, as we did with Android. This thesis is based on those works; we wanted to know if the results obtained were also applicable to Android malware.

To our knowledge, our work is the first to analyze the code evolution of Android malware from this perspective. We also believe that our dataset of Android malware source code is the largest analyzed in the literature.

The main findings of our work include:

\begin{itemize}
	\item In the Android malware landscape there is a high tendency towards spying malware, such as Spyware, RATs, Trojan-Spy, Keyloggers, etc.
	\item The number of malware samples increases at a rapid rate every year.
	\item Antivirus detection rates are severely skewed towards Lockers, Trojan-Bankers, Ransomware, Rootkits, Keyloggers, and RATs, as the rest of the malware types hardly raised a single detection.
	\item There is a high annual increase in the number of source code files, SLOCs, functions and programming languages used.
	\item There is a big difference in the number of files, SLOCs, functions and programming languages between malware types, as Backdoors, Trojan-Bankers and RATs often outnumber the other types in some of these categories.
	\item Android malware samples have a high value of effort, development time, and team size, which is steadily increasing every year.
	\item Android malware samples have a low value of complexity, maintainability index, and comment ratio, which slowly decreases every year.
\end{itemize}

This paper is structured as follows. In Section \ref{ch:dataset} we describe our malware source code dataset and the capabilities of the malware contained in it. Section \ref{ch:evolution_analysis} presents measurements on the evolution of malware development, in terms of size, cost and quality. In Section \ref{ch:comparison_malware} we compare the results of our Android malware with those obtained from non-specific malware. In Section \ref{ch:discussion} we discuss the limitations of our approach, and future work that we have set aside. Section \ref{ch:related_work} reviews previous similar work. Finally, Section \ref{ch:conclusion} concludes the paper.

\section{Dataset}
\label{ch:dataset}

To perform the analysis, an Android malware dataset is required. Due to the nature of this analysis, the dataset must be composed of open source samples, so binary or decompiled sources are discarded. Gathering malware in this format can prove to be extremely challenging. Not only is it difficult to obtain malware samples as-is, but they are often shared in their binary format; in the case of Android malware, the APK is the usual means of distribution. For the malware source code to be publicly available, the sample may have been open-sourced from the beginning, voluntarily opened later in the development or leaked. In the case of the source code samples we managed to find, most of them were either used in the wild and subsequently leaked or open-sourced early on as a functional proof-of-concept (PoCs). We have spent several months searching for as much Android malware source code as we could find. For this reason, we believe that during this process we have assembled the largest collection of Android malware source code in existence.

To accomplish this task, we scoured \textit{GitHub} \cite{b6} and public repositories for source code samples. One of the most interesting repositories is \textit{MalwareSourceCode} \cite{b7}, from \textit{vx-underground} \cite{b8}, as it contains many well-known leaked Android malware source code samples. It is likely that there are samples found in underground forums that have not been published on GitHub, but since we already had enough samples for the limited workforce we had, we did not venture into those forums. Nevertheless, it would be worthwhile to further this research with such samples to try to complete the dataset with them.

The dataset consists of malware samples. A sample is a specific variant of a malware project. There can be several variants of a specific malware project; they are said to share the same malware family. To distinguish between samples and variants, we have labeled each of them using a system similar to the CARO naming scheme \cite{b9}: \textbf{MalwareType:OperatingSystem/MalwareFamily.Variant}. As an example, one of the samples in our dataset is labeled as \textit{Trojan-Banker:AndroidOS/Anubis.k}.

In our dataset, the samples are bundled in ZIP archives in a GitHub repository \cite{b10}. A sample can consist of one or several source code files. In the case of Android malware, that code usually corresponds to the logic and views behind the malicious application to be installed, but may also involve the C2 server to which the malicious app will connect or the type of payload to be sent in between, as well as web page code to load at runtime; it will depend on the type of malware and its capabilities. Since the code may serve different purposes and systems, the source code files may be written in multiple programming languages and have different directory structures, but they will usually all share the same structure for the code of the Android application itself.

The 97 samples that make up our final dataset have been labeled with a unique ID and tagged with a year and a generic description of their behavior. The year corresponds to the last date of their development when indicated by the source, otherwise with the year they were uploaded to the GitHub repository. They are also labeled with a coarse-grained malware type that best matches their behavior and fine-grained malware categories (called tags in this research) into which they also fit. Table \ref{table:malware_types_distribution} shows the distribution of malware types in our dataset, while Table \ref{table:malware_tags_frequency} shows the frequency of different malware tags across all samples.

\begin{table}[htbp]
	\caption{Distribution of Malware Types}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Malware Type}& \textbf{No. Malware Samples} \\ \hline \hline
			RAT& 31 \\ \hline
			Spyware& 13 \\ \hline
			Trojan-Spy& 9 \\ \hline
			Keylogger& 8 \\ \hline
			Trojan-Banker& 7 \\ \hline
			Rootkit& 5 \\ \hline
			Locker& 4 \\ \hline
			Ransomware& 4 \\ \hline
			Phishing& 3 \\ \hline
			Trojan-SMS& 3 \\ \hline
			Dropper& 2 \\ \hline
			Trojan-Backdoor& 2 \\ \hline
			Backdoor& 1 \\ \hline
			Downloader& 1 \\ \hline
			Password-Stealing-Ware& 1 \\ \hline
			Scareware& 1 \\ \hline
			Trojan& 1 \\ \hline
			Trojan-Wiper& 1 \\ \hline
		\end{tabular}
		\label{table:malware_types_distribution}
	\end{center}
\end{table}

\begin{table}[htbp]
	\caption{Frequency of Malware Tags}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Malware Tag}& \textbf{No. Malware Samples} \\ \hline \hline
			Spyware& 72 \\ \hline
			Botnet& 60 \\ \hline
			Backdoor& 44 \\ \hline
			C2& 44 \\ \hline
			Billing-Fraud& 40 \\ \hline
			Trojan& 35 \\ \hline
			RAT& 34 \\ \hline
			Downloader& 31 \\ \hline
			Elevated-Privilege-Abuse& 27 \\ \hline
			Locker& 19 \\ \hline
			Keylogger& 17 \\ \hline
			Mailfinder& 12 \\ \hline
			Wiper& 12 \\ \hline
			Password-Stealing-Ware& 11 \\ \hline
			Phishing& 9 \\ \hline
			Encryption-Ransomware& 8 \\ \hline
			Screen-Locking-Ransomware& 8 \\ \hline
			Overlay& 7 \\ \hline
			Clicker& 6 \\ \hline
			Loader& 6 \\ \hline
			Dropper& 5 \\ \hline
			Rootkit& 5 \\ \hline
			Rooting& 3 \\ \hline
			DoS& 2 \\ \hline
			Proxy& 2 \\ \hline
			Spam& 2 \\ \hline
			Scareware& 1 \\ \hline
		\end{tabular}
		\label{table:malware_tags_frequency}
	\end{center}
\end{table}

To make these distinctions and correctly label each sample, we manually reviewed the source code of all samples and noted which malware tags they fit into and which individual malware type best described them, as well as the aforementioned description and other data, such as permissions and capabilities, which further aided in the classification of the samples.

As can be seen in Table \ref{table:malware_types_distribution}, most of the samples collected are related to spying on the victim. In fact, the five most abundant malware types in our dataset are spyware variants (RAT, Spyware, Trojan-Spy, Keylogger and Trojan-Banker). This fact should not come as a surprise, since smartphones are the most coveted targets for this type of malware, as the feedback obtained from these devices is richer than that of any desktop. After all, cell phone users always carry it everywhere, so the information obtained from the GPS location, camera or microphone will be more abundant and accurate. Another benefit of spying on smartphones is gaining access to calls and SMS (useful for 2FA, spam or billing fraud), instant messaging, contacts, and email and bank accounts, further enhancing the capabilities of mobile spyware compared to its desktop counterpart. Also noteworthy is the appearance of rootkits, lockers and ransomware (Android ransomware comes in two variants, both file-encrypting and device-locking).

The malware tags also reveal that nearly  75\% have some spyware capability. It also shows that 60 of the 97 samples are used to create a botnet; 44 out of those 60 use a remote C2 server to manage it, while others use SMS, email or other means to administer their bots. It is also apparent that 40 samples can perform billing fraud by either sending SMS or calls without the user's knowledge. 31 samples can download files from the Internet, some of which subsequently install those files as applications or are used to update the malware itself to gain more capabilities. It is also worth noting that 7 of the total samples perform overlay attacks, where the malicious app injects its own view on top of a legitimate one when the victim opens a specific app; this attack is mostly used by banking Trojans to overlay on top of legitimate banking apps and steal the victim's credentials.

Figure \ref{fig:year_distribution} shows the year distribution of the final dataset of 97 samples. Approximately 50\% of the samples correspond to the last three years (2020-2022). The second largest set of samples consists of about 40\% of the overall samples, corresponding to the period 2016-2019. Finally, the remaining samples, ranging form 2012 to 2015, account for less than 15\% of all samples. It is interesting to note that almost 30\% of all samples were developed in 2021. The linear regression clearly depicts the increase in the number of malware over the years, as it increases \textit{1.45} samples each year. To measure how well the proportion of variation in the samples is described by the linear regression, we use the coefficient of determination, also known as R\textsuperscript{2}. This value ranges from 0 to 1; the closer it is to 1, the better the linear regression fits the sample values. In the case of the malware distribution by year, this coefficient has a value of \textit{.47}, which means that approximately half of the variation can be explained by the regression.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={No. Malware Samples},
			xtick=data,
			ymin = 0, ymax = 35,
			x tick label style={rotate=90,anchor=east},
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[ybar, ybar legend, blue, fill=blue!30!white, nodes near coords, nodes near coords align={vertical}] table[x = Year, y = Count] {data/years.dat};
			\addplot[thick, orange,domain=2012:2022]{1.4455*x-2906.6636};
			\addlegendentry{Frequency}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{1.4455}
				\cdot x
				\pgfmathprintnumber[print sign]{-2906.6636}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Malware Distribution by Year}
	\label{fig:year_distribution}
\end{figure}

Table \ref{table:malware_permissions_frequency} shows the most common permissions requested by the malicious apps that make up our dataset. There are 10 samples that had no permissions, as they are not Android applications (scripts written in Python, kernel rootkits in C, etc.), or whose \textit{AndroidManifest.xml} file was not found. To extract the permissions automatically, a Python tool was developed \cite{b11}. It can also extract package names and actions from the manifest, and imports from the code, as they can give the researcher a general idea of the project's capabilities. It also has the ability to add information to each permission (description, group, group description, protection level and deprecation status), pulling it from a JSON file that we developed by aggregating public information and reports from \textit{Android Developers} \cite{b12} and other sources in what is arguably the most extensive public Android permission database to date, containing both new and deprecated permissions from older Android versions. It is also capable of counting the frequency of permissions and imports across various projects and displaying a comprehensive graph with that information.

\begin{table}[htbp]
	\caption{Frequency of Malware Permissions}
	\scriptsize
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Permission}& \textbf{No. Malware Samples} \\ \hline \hline
			INTERNET& 66 \\ \hline
			RECEIVE\_BOOT\_COMPLETED& 55 \\ \hline
			WRITE\_EXTERNAL\_STORAGE& 51 \\ \hline
			READ\_SMS& 47 \\ \hline
			ACCESS\_NETWORK\_STATE& 45 \\ \hline
			READ\_PHONE\_STATE& 44 \\ \hline
			READ\_CONTACTS& 44 \\ \hline
			SEND\_SMS& 39 \\ \hline
			WAKE\_LOCK& 35 \\ \hline
			ACCESS\_FINE\_LOCATION& 35 \\ \hline
			RECEIVE\_SMS& 33 \\ \hline
			RECORD\_AUDIO& 31 \\ \hline
			READ\_EXTERNAL\_STORAGE& 31 \\ \hline
			CAMERA& 31 \\ \hline
			CALL\_PHONE& 26 \\ \hline
			READ\_CALL\_LOG& 24 \\ \hline
			ACCESS\_COARSE\_LOCATION& 21 \\ \hline
			SYSTEM\_ALERT\_WINDOW& 20 \\ \hline
			BIND\_ACCESSIBILITY\_SERVICE& 18 \\ \hline
			VIBRATE& 16 \\ \hline
			BIND\_DEVICE\_ADMIN& 16 \\ \hline
			GET\_TASKS& 14 \\ \hline
			ACCESS\_WIFI\_STATE& 14 \\ \hline
			WRITE\_SETTINGS& 13 \\ \hline
			WRITE\_CONTACTS& 13 \\ \hline
			FOREGROUND\_SERVICE& 13 \\ \hline
			BIND\_NOTIFICATION\_LISTENER\_SERVICE& 13 \\ \hline
			WRITE\_SMS& 12 \\ \hline
			REQUEST\_IGNORE\_BATTERY\_OPTIMIZATIONS& 12 \\ \hline
			PROCESS\_OUTGOING\_CALLS& 12 \\ \hline
			GET\_ACCOUNTS& 12 \\ \hline
			READ\_HISTORY\_BOOKMARKS& 10 \\ \hline
			WRITE\_CALL\_LOG& 10 \\ \hline
			BROADCAST\_SMS& 10 \\ \hline
		\end{tabular}
		\label{table:malware_permissions_frequency}
	\end{center}
\end{table}

In this case, the most frequent permission in all samples is \textit{Internet} access request. This permission is so common because, as seen before, most samples are spyware related, and therefore need to exfiltrate data remotely. The second most common permission is used to receive a notification when the boot process has finished. This is leveraged to launch the application each time the notification is received, therefore the app will always run at startup. This is done to add persistence; in the case of spyware to always remain in the background collecting information, and in the case of ransomware or lockers, to ensure that the device remains inaccessible even after reboot.  Another permission is used to read the \textit{phone state}, which is used to get cellular network information and know when phone calls are being made. Lockers usually draw their own view on top of everything else to prevent the victim from using their phone, and phishing malware also use this technique to inject their view into legitimate ones to ask for credentials in a credible manner; this can be done by requesting the \textit{SYSTEM\_ALERT\_WINDOW} permission. The \textit{GET\_TASKS} permission is also commonly used with it to check which applications are in the foreground an correctly inject specific views over them. Most keyloggers use \textit{BIND\_ACCESSIBILITY\_SERVICE} to capture keystrokes, and other malware also request this permission to read the text present on the screen, alert on transitions between applications, and spoof clicks and presses (clicker, clickjacking) which allow the app to grant itself permissions, click on specific ads to generate revenue for the developer, and more. To escalate privileges, some malware requests \textit{BIND\_DEVICE\_ADMIN}, which provides administration features. Another interesting permission is \textit{REQUEST\_IGNORE\_BATTERY\_OPTIMIZATIONS}, as it prevents system optimizations from killing the app if it runs in the background during Doze mode, ensuring its persistance. There are also a large number of permissions that request access to SMS, contacts, location, camera, call log, etc. These permissions clearly depict the main interests of spyware samples.

After manual review of each sample, we collected all the capabilities that were inferred from the source code. Tables \ref{table:malware_capabilities_I} and \ref{table:malware_capabilities_II} are the result of counting the frequency of occurrence of each capability and joining them in one table. In this case, the table is split in two, as it did not fit on a single page. Both tables only show the most frequent capabilities, as the less common ones did not reflect the entire dataset. The most frequent capabilities show profound similarities to the Tables \ref{table:malware_tags_frequency} and \ref{table:malware_permissions_frequency}; after all, permissions and malware types are closely related to malware capabilities.

\begin{table}[htbp]
	\caption{Frequency of Malware Capabilities I}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Capability}& \textbf{No. Malware Samples} \\ \hline \hline
			Internet& 69 \\ \hline
			Remote Data Exfiltration& 61 \\ \hline
			Receive Boot Completed& 58 \\ \hline
			Run at Startup& 58 \\ \hline
			Run in Background& 57 \\ \hline
			Bot& 54 \\ \hline
			Write External Storage& 53 \\ \hline
			Read SMS& 50 \\ \hline
			Access Network State& 49 \\ \hline
			Device Information& 47 \\ \hline
			Read Contacts& 46 \\ \hline
			Read Phone State& 44 \\ \hline
			Send SMS& 41 \\ \hline
			Access Fine Location& 39 \\ \hline
			Wake Lock& 38 \\ \hline
			Receive SMS& 35 \\ \hline
			Upload Files& 35 \\ \hline
			Camera& 32 \\ \hline
			Read External Storage& 32 \\ \hline
			Record Audio& 31 \\ \hline
			Stealth& 31 \\ \hline
			Download Files& 30 \\ \hline
			Call Phone& 27 \\ \hline
			Icon Hiding& 27 \\ \hline
			Create Files& 26 \\ \hline
			Access Coarse Location& 25 \\ \hline
			Delete Files& 25 \\ \hline
			Persistance& 25 \\ \hline
			Read Call Log& 25 \\ \hline
			Privilege Escalation& 23 \\ \hline
			System Alert Window& 22 \\ \hline
			List Files& 20 \\ \hline
			Lock Device& 20 \\ \hline
			Periodical Connection With Server& 20 \\ \hline
			Activate Admin& 19 \\ \hline
			Input Capture& 18 \\ \hline
			Vibrate& 18 \\ \hline
			Bind Accessibility Service& 17 \\ \hline
			Keystrokes Monitoring& 17 \\ \hline
		\end{tabular}
		\label{table:malware_capabilities_I}
	\end{center}
\end{table}

\begin{table}[htbp]
	\caption{Frequency of Malware Capabilities II}
	\begin{center}
		\begin{tabular}{|c|c|}
			\hline
			\textbf{Capability}& \textbf{No. Malware Samples} \\ \hline \hline
			List Installed Apps& 17 \\ \hline
			Make Toasts& 17 \\ \hline
			Access Wifi State& 16 \\ \hline
			Bind Device Admin& 16 \\ \hline
			Data Encoding& 15 \\ \hline
			Open URL& 15 \\ \hline
			Check Screen Locked& 14 \\ \hline
			Bind Notification Listener Service& 13 \\ \hline
			Foreground Service& 13 \\ \hline
			Get Tasks& 13 \\ \hline
			Write Contacts& 13 \\ \hline
			Write Settings& 13 \\ \hline
			Credential Theft& 12 \\ \hline
			Get Accounts& 12 \\ \hline
			Read Files& 12 \\ \hline
			Read History Bookmarks& 12 \\ \hline
			Remote Shell& 12 \\ \hline
			Root Check& 12 \\ \hline
			Write SMS& 12 \\ \hline
			Connectivity Check& 11 \\ \hline
			Data Decoding& 11 \\ \hline
			Open Apps& 11 \\ \hline
			Persist on Screen& 11 \\ \hline
			Process Outgoing Calls& 11 \\ \hline
			Screenshot& 11 \\ \hline
			Broadcast SMS& 10 \\ \hline
			Change Wifi State& 10 \\ \hline
			Draw Over Other Apps& 10 \\ \hline
			Local Data Exfiltration& 10 \\ \hline
			Uninstall Itself& 10 \\ \hline
			Write Call Log& 10 \\ \hline
			Write Files& 10 \\ \hline
			File Decryption& 9 \\ \hline
			File Encryption& 9 \\ \hline
			Install Apps& 9 \\ \hline
			Logging& 9 \\ \hline
			Request Ignore Battery Optimizations& 9 \\ \hline
			Uninstall Apps& 9 \\ \hline
			Volume& 9 \\ \hline
		\end{tabular}
		\label{table:malware_capabilities_II}
	\end{center}
\end{table}

The \textit{device information} capability refers to malware samples being able to extract the IMEI number, manufacturer, hardware specifications, etc., which are often employed to uniquely identify the infected device in order to properly distinguish it from other components of a botnet. Most malware also has the ability to manage internal files, such as \textit{creating}, \textit{modifying} and \textit{deleting} them. A C2 server can also prompt the infected device to \textit{list the files} present in a folder and \textit{upload} them to the server, or \textit{download files} from the server or the Internet. This can then be used to \textit{install new applications}. Other malware has the ability to exfiltrate the \textit{list of installed apps}, \textit{screenshots}, \textit{open specific applications} or \textit{open URLs} in the default browser. Some have the ability to \textit{hide the app icon} so that it is not visible from the launcher, while others have the ability to \textit{uninstall applications}, even \textit{themselves} so as to not leave any traces. It is also common for malware to check if \textit{Internet connectivity} is available or if the phone is \textit{rooted} in order to perform privilege actions, such as executing root commands on a terminal, as some malware has the ability to open a \textit{remote shell}.

Another interesting approach to our dataset is to check whether antivirus software flags newer Android malware as malicious more often than older samples. To this end, we used the online service VirusTotal \cite{b13} to upload the ZIP files and wrote down the detection rate of each sample as a percentage of the number of antivirus solutions that flagged the sample as malware. Overall, our dataset has an average detection rate of \textit{8.026\%}. As shown in Figure \ref{fig:vt_score}, there is a slight increase in detections, which turns out to be almost constant over the years. In this case, the coefficient of determination has a value equal to \textit{.0026}, which implies that the linear regression does not fit the data properly, as the values of each sample are extremely scattered.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={VirusTotal Score (\%)},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 65,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = vscore] {data/vscore.dat};
			\addplot[thick, orange,domain=2012:2022]{0.2929*x-583.2226};
			\addlegendentry{VirusTotal Score}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{0.2929}
				\cdot x
				\pgfmathprintnumber[print sign]{-583.2226}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{VirusTotal Score by Year}
	\label{fig:vt_score}
\end{figure}

To look at detection rates from a different perspective, instead of sorting the number of VirusTotal detections by year, we arranged them by malware type. Figure \ref{fig:vt_type} shows that there is indeed a clear imbalance between the different malware types, which can be divided into four blocks. Lockers (LCK), Trojan-Bankers (TBK) and Ransomware (RNM) samples raise by far the most alerts. The second most detected block is made up of Rootkits (RTK), Keyloggers (KLG) and RATs (RAT). Next, Droppers (DRP), Spyware (SPY) and Trojan-Spy (TSY) samples are detected occasionally, but do not have a high rate. Finally, the remaining types of malware are not detected even once. One hypothesis that could explain this imbalance is that the noisier and pernicious malware types have been more vocal and are therefore more reported than the stealthier samples. It could also be due to the fact that their behavior is more distinct compared to that of the benevolent software. Another possibility is that most of the samples of the detected malware types posted on GitHub have been used in the wild, while the other types of malware found in public repositories tend to be PoCs and the like. Unfortunately, this thesis does not focus on obtaining an answer to this conundrum, so we will leave it with these speculations.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			ybar,
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={Avg. No. of VirusTotal Detections},
			xtick=data,
			x tick label style={font=\scriptsize,rotate=90,anchor=east},
			symbolic x coords={LCK, TBK, RNM, RTK, KLG, RAT, DRP, SPY, TSY, BCK, DWN, PSW, PSH, SCR, TJN, TBC, TMS, TWP},
			bar width=5pt
			]
			\addplot table[x = type, y = detections] {data/malwarevscore.dat};
		\end{axis}
	\end{tikzpicture}
	\caption{Average Number of VirusTotal Detections by Malware Type}
	\label{fig:vt_type}
\end{figure}

\section{Malware Evolution Analysis}
\label{ch:evolution_analysis}

This section outlines our analysis of the evolution of Android malware source code employing software metrics. It first quantifies the evolution in \nameref{section:code_size}, then it estimates \nameref{section:development_cost}, and it finally measures \nameref{section:code_quality}. In each section, we briefly introduce the software metrics  and methodology used, and refer the reader to the papers on which this thesis is based for more details \cite{b4}, \cite{b5}.

\subsection{Code Size}
\label{section:code_size}

We utilize 3 different metrics to measure code size: \nameref{subsection:number_files}, \nameref{subsection:number_lines} (SLOCs), and \nameref{subsection:number_functions}. We also measure the use of different \nameref{subsection:languages} in malware development.

\subsubsection{\textbf{Number of files}}
\label{subsection:number_files}

To count the number of files, we use \textit{Cloc} \cite{b14}, an easy-to-use open source tool that has a wide range of recognized languages and simultaneously calculates the number of files present in a given project, the blank lines, comment lines, and source lines of code broken down by programming language, which were also useful for the measurements in the following sections. When counting files, it discards binary files and repeated source code files.

Collectively, our dataset has an average of \textit{256.74} files per sample, totaling to \textit{24904} files. Figure \ref{fig:files_year} shows the distribution over time of the number of files comprising the source code for each sample in the dataset. With 2 exceptions, no malicious code exceeds 100 files before 2016. From 2016 to 2019, there are 5 samples exceeding 100 files, one of which almost manages to approach 1000 files. From 2020 onward, several samples surpass that limit, with one of them reaching 6024 files. In this range of years, 21 samples exceed 100 files. Only in the years 2017, 2019 and 2020 no sample falls bellow 10 files. Only 1 malware consists of 1 file: a Rootkit named \textit{Mindtrick}, written in C in 2015.

Therefore, the increase in the number of files per year is evident. The linear regression even shows that for each year, the number of files is increased by \textit{50.3}. This implies that, every two years, the size increases by 100 files. The coefficient of determination for this estimation has a value of \textit{.031}, which is somewhat low, as there are 3 samples that fall outside the norm when compared to the other samples: \textit{AhMyth} (6024 files), \textit{Bootloader-Backdoor} (3700 files) and \textit{Arbitrium} (2467 files). Although the linear regression does not perfectly represent the real values, there is no doubt that the number of files has increased in recent years.

Regarding \textit{Bootloader-Backdoor}, most of its files are C/C++ code, PO files, shell and Python scripts, HTML, XML, Kotlin and Java code, most of which are external tools and libraries that were imported and used by the malware. In the case of \textit{AhMyth} and \textit{Arbitrium}, since they are RAT samples, their files are mostly related to the server front end: JavaScript, CSS, HTML, JSON, TypeScript, Markdown and Java.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={No. of Files},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 7000,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = files] {data/files.dat};
			\addplot[thick, orange,domain=2012:2022]{50.2912*x-101262.9533};
			\addlegendentry{No. of Files}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{50.2912}
				\cdot x
				\pgfmathprintnumber[print sign]{-101262.9533}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Number of Files by Year}
	\label{fig:files_year}
\end{figure}

Figure \ref{fig:files_type} shows the samples organized by malware type rather than by year. Note that the y-axis is labeled on a logarithmic scale, so the difference between the maxima of the bars is larger that it may appear at first glance. The backdoor type has the most average number of files out of all malware types by a wide margin, as the only sample corresponding to this type is the aforementioned \textit{Bootloader-Backdoor} (3700 files). Next are Trojan-Bankers, as they use overlay techniques to inject malicious views on top of legitimate ones and need code that mimics those views as well as the logic behind them. They are followed by RATs, which need both client and server code to function properly. Most of the latest types of malware shown in the graph rely on a small number of files because they do not need more to perform their functions: Rootkits only need a file to hook system calls, Scareware only requires displaying intimidating messages to scare the victim, Droppers and Trojan-Backdoors are just the first step to deploy larger and more complex malware, etc.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			ybar,
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ymode=log,
			ylabel={Avg. No. of Files},
			xtick=data,
			x tick label style={font=\scriptsize,rotate=90,anchor=east},
			symbolic x coords={BCK, TBK, RAT, KLG, RNM, TSY, SPY, LCK, PSH, PSW, TWP, DWN, TMS, TJN, DRP, TBC, SCR, RTK},
			bar width=5pt
			]
			\addplot table[x = type, y = files] {data/malwarefiles.dat};
		\end{axis}
	\end{tikzpicture}
	\caption{Average Number of Files by Malware Type}
	\label{fig:files_type}
\end{figure}

\subsubsection{\textbf{Number of source code lines}}
\label{subsection:number_lines}

In order to measure the size of software programs, the most commonly used metric is to count the number of lines present in the source code, excluding blank lines and comments, which is called SLOCs (source lines of code). To obtain this number from the samples in our dataset, we use \textit{Cloc} \cite{b14}, the same tool used in the previous section. When measuring the SLOCs of a sample, we consider all the code in it, regardless of the programming language or its functionality. The average SLOCs per sample is \textit{36481.27}, adding up to \textit{3538683} SLOCs. Figure \ref{fig:slocs_year} is the result of sorting the measured SLOCs in years.

Similar to the number of files, the growth of SLOCs is evident. Up to 2015, the \textit{AndroidSurveillance} RAT is the biggest sample in terms of SLOCs (28688), while the smallest is once again the \textit{Mindtrick} Rootkit (68). During this time period, only 3 samples exceed 3100 lines of code, while there are 5 samples bellow 300 SLOCs. Between 2016 and 2019, no sample falls under 300 SLOCs, while 10 samples exceed 3100 lines of code. The largest sample during this time, from the \textit{Slempo} Trojan-Banker family, reaches one hundred thousand SLOCs (160267). From 2020 to date, several samples exceed the hundred thousand mark and one of them even reaches one million SLOCs. The three largest samples in terms of SLOCs are: \textit{Bootloader-Backdoor} (2021) with 1037561 lines, \textit{AhMyth} (2020) with 761,610 and \textit{Cerberus} (2020) with 220,173.

This analysis bears a strong resemblance to the number of files. The same pattern is present in this case, and even two out of the three largest samples are the same; some of the smaller samples also remain unchanged. The linear regression shows an increase of \textit{6710.39} SLOCs per year. Moreover, the coefficient of determination is equal to \textit{.018}, which makes it clear that there is a very large variance in the SLOCs in our dataset.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={SLOCs},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 1200000,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = slocs] {data/slocs.dat};
			\addplot[thick, orange,domain=2012:2022]{6710.3865*x-13509367.85};
			\addlegendentry{SLOCs}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{6710.3865}
				\cdot x
				\pgfmathprintnumber[print sign]{-13509367.85}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Number of SLOCs by Year}
	\label{fig:slocs_year}
\end{figure}

Sorting the SLOCs by malware type, the resulting graph (Figure \ref{fig:slocs_type}) closely resembles the one presented in the previous section (Figure \ref{fig:files_year}). The 6 largest malware types remain almost unchanged, while the smallest ones experience a slight shift. As expected, there is a strong relationship between files and SLOCs, so the conclusions in terms of sustained growth are similar.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			ybar,
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ymode=log,
			ylabel={Avg. SLOCs},
			xtick=data,
			x tick label style={font=\scriptsize,rotate=90,anchor=east},
			symbolic x coords={BCK, TBK, RAT, RNM, KLG, TSY, PSH, SPY, LCK, DRP, SCR, PSW, TWP, TBC, DWN, RTK, TMS, TJN},
			bar width=5pt
			]
			\addplot table[x = type, y = slocs] {data/malwareslocs.dat};
		\end{axis}
	\end{tikzpicture}
	\caption{Average SLOCs by Malware Type}
	\label{fig:slocs_type}
\end{figure}

\subsubsection{\textbf{Number of functions}}
\label{subsection:number_functions}

Although SLOCs is the most popular metric for measuring project size, it is not foolproof, as it does not take into account that different programming languages produce different lengths of code for the same functionality, as they are different in terms of expressiveness. For that reason, we also decided to count the number of functions present in the code, as it not only illustrates the size of the project, but can also lead to insightful conclusions in terms of code quality, as it is generally better to split the code into smaller functions, as it improves legibility, maintainability and avoids repetition of commonly used code.

To count the number of functions per sample, we use the \textit{Unified Code Count - Java (UCC-J)} \cite{b15}. This is a code metrics tool developed by the \textit{Boehm Center for Systems and Software Engineering} \cite{b16} that allows to count physical and logical lines of code, obtain Cyclomatic Complexity and Maintainability Index results, count functions, and more. This specific release supports different languages: Ada, ASP/ASP.NET, Bash, C/C++, C Shell Script, COBOL, ColdFusion, ColdFusion Script, CSS, C\#, DOS Batch, Fortran, Go, HTML, Java, JavaScript, JSP, Makefiles, MATLAB, NeXtMidas, Pascal, Perl, PHP, Python, R, Ruby, Scala, SQL, VB, VBScript, Verilog, VHDL, XML, and XMidas. Unfortunately, it does not support Kotlin, so any samples containing code written in Kotlin will be discarded. We took note of the number of functions detected, as well as the number of files form which those functions were extracted, as we will review the average number of functions per file later. Summing over the entire number of functions present in our dataset uncovers \textit{20813} functions, with an average of \textit{233.85} per sample.

Figure \ref{fig:functions_year} reveals yet another correlation between the number of files, SLOCs and number of functions, as this graph continues to look similar to the previous ones. In this case, however, the values are slightly more balanced between the years. Already in 2012, the \textit{AndroRAT} malware has 890 functions, which is almost 4 times the average. Apart from this rarity, the range of years from 2013 to 2015 is bellow average, where the lowest sample has 2 functions (\textit{FakeFacebook}) and the largest one has 265 (\textit{Dendroid}), which is the only one that manages to exceed the average value. From 2016 to 2019, four samples exceed the average number of functions. The lowest value in this period is 7 (\textit{Rootkit-Android}), and the highest is 645 (\textit{GmBot}). Finally, the period between 2020 and 2022 has the samples with the highest number of functions. These are \textit{Lokiboard} (2020), with 1283, \textit{Apps\_Keylogger} (2021), with 1283, \textit{Lokiboard-mod} (2020), with 1350, and \textit{Covid-Locker} (2021), with 5846. In this case, these are three Keyloggers of the same family and a Ransomware. None of these samples stood out this much in the previous analyses; this is possibly due to the fact that this tool is limited in the number of languages it can interpret, so there are some samples with a high volume of code written in these non-interpreted languages that are not being counted.

Linear regression suggests that there is an increase of \textit{34.24} functions each year. The coefficient of determination stands at \textit{.02}, which, again, is a low score, indicative of the variability in the data.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={No. of Functions},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 6600,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = functions] {data/functions.dat};
			\addplot[thick, orange,domain=2012:2022]{34.2425*x-68883.2897};
			\addlegendentry{No. of Functions}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{34.2425}
				\cdot x
				\pgfmathprintnumber[print sign]{-68883.2897}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Number of Functions by Year}
	\label{fig:functions_year}
\end{figure}

Figure \ref{fig:functions_avg_year} shows the average number of functions per file. As mentioned above, in general it is better to have fewer functions per file to improve readability and maintenance of the code. As can be seen, there are a few files that deviate from the norm, but most of them are close to the overall average, which is close to \textit{7.33}. The most outlier samples are \textit{BetterAndroRAT} (2016), \textit{Dendroid} (2014), \textit{rdroid} (2018), and \textit{Android-Rootkit} (2015). From 2019 to 2022 most samples fall bellow average. Linear regression shows a decrease of \textit{0.13} and a coefficient of determination equal to \textit{.0054}, as the data are highly balanced and scattered.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={Avg. No. of Functions Per File},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 32,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = functionsavg] {data/functionsavg.dat};
			\addplot[thick, orange,domain=2012:2022]{-0.1286*x+266.985};
			\addlegendentry{Average Number of Functions Per File}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{-0.1286}
				\cdot x
				\pgfmathprintnumber[print sign]{266.985}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Average Functions Per Files by Year}
	\label{fig:functions_avg_year}
\end{figure}

\subsubsection{\textbf{Programming languages}}
\label{subsection:languages}

%r=.058
Figure \ref{fig:languages_year} shows the year distribution of the number of distinct languages used to develop each malware sample. This includes compiled and interpreted languages, such as C\#, C/C++, Java, Kotlin, PHP, Python, or JavaScript, languages used to construct resources, which include HTML, XML, and CSS, and scripts used to build the project, that is, BAT or Make files. To obtain the languages for each project, we use \textit{Cloc} \cite{b14}, which was also used for the number of files and SLOCs.

The languages are mostly arranged around the linear regression, having only two far outlayer samples: \textit{AhMyth} (2020) and \textit{Arbitrium}(2021), both of which are RAT malware and were mentioned in the \nameref{subsection:number_files} section, as these malware samples make heavy use of different server-side technologies and languages. The average number of languages per project is \textit{8.75}. This is a large number, since Android applications are usually developed with at least 5 languages: Java/Kotlin, XML, Gradle, Text and Properties. Linear regression suggests that every 2 years, another language is added to the samples, as the slope has a value of \textit{0.56}. Once again, the coefficient of determination has a low value: \textit{0.58}.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={No. Languages},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 60,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = langs] {data/langs.dat};
			\addplot[thick, orange,domain=2012:2022]{0.5641*x-1130.0499};
			\addlegendentry{Languages}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{0.5641}
				\cdot x
				\pgfmathprintnumber[print sign]{-1130.0499}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Number of Different Languages used by Year}
	\label{fig:languages_year}
\end{figure}

\subsection{Development Cost}
\label{section:development_cost}

The \textit{Constructive Cost Model} (\textit{COCOMO}) \cite{b17} is a reliable regression model based on source code lines to measure and predict software project development cost metrics, such as \nameref{subsection:effort}, \nameref{subsection:time}, \nameref{subsection:size} and \nameref{section:code_quality}. Effort corresponds to the amount of labor that is required to complete the project (measured in man-months). Development time is the estimated time required to complete the project (measured in months). Team size is an estimated of the people required to develop the project (measured in persons). The following equations represent the three metrics:

\[ E = a(KLOC)^{b}  \]
\[ D = cE^{d}  \]
\[ P = \frac{E}{D}  \]

In the preceding equations, KLOC represent the estimated SLOCs in thousands and a, b, c, and d are constant values obtained empirically and provided by the model as a function of the characteristics and nature of the project (Table \ref{table:COCOMO_constants}). Three types of projects where considered for this model:

\begin{itemize}
	\item Organic: small team, good experience, low complexity, flexible software requirements.
	\item Semi-detached: medium-sized team, mixed experience, medium complexity, combination of rigid and flexible requirements. 
	\item Embedded: large team, high level of experience, high complexity, rigid software requirements.
\end{itemize}

For our dataset, we consider that all samples fall into the Organic project category, as malware development is usually led by small teams of programmers, and for this project we are inclined to conservatively estimate development costs rather than overestimate them.

\begin{table}[htbp]
	\caption{COCOMO constants}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\textbf{Software Projects}& \textbf{a}& \textbf{b}& \textbf{c}& \textbf{d} \\ \hline \hline
			Organic&	2.4&	1.05&	2.5&	0.38\\ \hline
			Semi Detached&	3.0&	1.12&	2.5&	0.35\\ \hline
			Embedded&	3.6&	1.20&	2.5&	0.32\\ \hline
		\end{tabular}
		\label{table:COCOMO_constants}
	\end{center}
\end{table}

\subsubsection{\textbf{Effort}}
\label{subsection:effort}

Figure \ref{fig:effort_year} shows the COCOMO estimate of effort for each sample over the years. The global average effort is \textit{115.98}, and the linear regression has a slope of \textit{22.09}, showing that the effort increases rapidly each year. There are two samples (\textit{Bootloader-Backdoor} and \textit{AhMyth}) that probably do not fit into the Organic project category, as there is a large discrepancy between their values and the rest. Therefore, the coefficient of determination shows a low value, \textit{.017}. Nevertheless, the value of the effort increases steadily each year.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={Estimated Effort (man-moths)},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 4000,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = effort] {data/effort.dat};
			\addplot[thick, orange,domain=2012:2022]{22.0902*x-44476.1414};
			\addlegendentry{Effort Score (man-months)}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{22.0902}
				\cdot x
				\pgfmathprintnumber[print sign]{-44476.1414}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Estimated Effort by Year (man-months)}
	\label{fig:effort_year}
\end{figure}

\subsubsection{\textbf{Development time}}
\label{subsection:time}

Similar to the effort metric, the estimated time to develop samples present in our dataset (Figure \ref{fig:time_year}) shows the same two samples deviating from the norm. The average development time is \textit{8.32}, and these samples have an estimate 6 times larger than the average. Linear regression shows that, starting from 3.96 months in 2012, the value has transformed to 10.65 months in 2022, as its coefficient is \textit{0.67} months per year. The coefficient of determination is \textit{.038}.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={Estimated Development Time (months)},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 65,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = time] {data/time.dat};
			\addplot[thick, orange,domain=2012:2022]{0.6693*x-1342.6702};
			\addlegendentry{Development Time (months)}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{0.6693}
				\cdot x
				\pgfmathprintnumber[print sign]{-1342.6702}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Estimated Development Time by Year (months)}
	\label{fig:time_year}
\end{figure}

\subsubsection{\textbf{Team size}}
\label{subsection:size}

As both previous metrics have shown, the estimated team size (Figure \ref{fig:size_year}) also depicts the two outlier malware samples. In this case, the earlier malware required about 3 persons (full time). In 2019, the average (\textit{4.31}) is exceeded, and in 2022 it is estimated that 6 persons are necessary to develop a malware sample. The increase per year is \textit{0.67}, which can be translated into 2 persons per 3 years. The coefficient of determination is \textit{0.26}.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={Estimated Team Size (persons)},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 75,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = team] {data/team.dat};
			\addplot[thick, orange,domain=2012:2022]{0.5562*x-1118.5223};
			\addlegendentry{Team Size (persons)}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{0.5562}
				\cdot x
				\pgfmathprintnumber[print sign]{-1118.5223}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Estimated Team Size by Year (persons)}
	\label{fig:size_year}
\end{figure}

\subsection{Code Quality}
\label{section:code_quality}

In this section, we measure three aspects of code quality: \nameref{subsection:complexity}, \nameref{subsection:maintainability} and \nameref{subsection:comments} of the samples.

\subsubsection{\textbf{Complexity}}
\label{subsection:complexity}

To measure software complexity of our samples, we use McCabe’s \textit{cyclomatic complexity} \cite{b18}, one of the most commonly used software complexity metrics. The cyclomatic complexity (CC) of source code is computed from its control flow graph, and measures the number of linearly independent paths within the graph, taking into account the number graph nodes (N), edges (E), and the connected components (P):

\[ CC = E-N+2P \]

To calculate the cyclomatic complexity, we use the \textit{Universal Code Count} (\textit{UCC-J}) \cite{b15} (the same tool used in \nameref{subsection:number_functions}), as it supports a wide range of languages, which is extremely useful for our analysis, since Android malware samples use many diverse languages for a single project. Still, it is not compatible with Kotlin source code, which appears in many projects in our dataset that were therefore excluded from this analysis.

Figure \ref{fig:samples_complexity} illustrates the distribution of the average cyclomatic complexity per function for each malware sample. Most of the projects have functions with complexities ranging from 1 to 3. The most common range of values is [1,2], and the average of all samples is \textit{2.29}. Overall, this can be seen as evidence in support of a generally monolithic design with poor break down into simple functions. However, there are some counterexamples. There is a sample with complexity equal to 6, called \textit{SARA} (Ransomware, 2022), and one equal to 8, called \textit{Adore} (Rootkit, 2014).

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			ybar,
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={No. Malware Samples},
			xlabel={Cyclomatic Complexity Values},
			symbolic x coords={1-2, 2-3, 3-4, 4-5, 5-6, 6-7, 7+},
			xtick=data,
			nodes near coords, 
			nodes near coords align={vertical}
			]
			\addplot table[x = range, y = count] {data/complexityagg.dat};
		\end{axis}
	\end{tikzpicture}
	\caption{Number of Samples By Cyclomatic Complexity Values}
	\label{fig:samples_complexity}
\end{figure}

Looking at the annual distribution of the cyclomatic complexity values (Figure \ref{fig:complexity_year}), it can be observed that the complexity decreases slightly each year, at a rate of \textit{-0.0322}. The coefficient of determination, which is \textit{.0078}, shows a scattered set of values.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={Complexity},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 10,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = complexity] {data/complexity.dat};
			\addplot[thick, orange,domain=2012:2022]{-0.0322*x+67.2858};
			\addlegendentry{Complexity Score}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{-0.0322}
				\cdot x
				\pgfmathprintnumber[print sign]{67.2858}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Cyclomatic Complexity by Year}
	\label{fig:complexity_year}
\end{figure}

\subsubsection{\textbf{Maintainability}}
\label{subsection:maintainability}

Another useful metric related to software quality is the maintainability of source code. There are several reasons to maintain code, as it can be improved, fixed or extended. Maintainability can be measured with the maintainability index (MI), which is an estimate of how easy it is to comprehend, sustain and alter code. Both maintainability and complexity are often related, as more complex code translates into a low maintainability index.

To obtain the MI, we use \textit{UCC-J} \cite{b15}, as it calculates it automatically by function. In the case of the MI, this tool only calculates it for Java code, which further decreased the number of samples analyzed, as 17 of them were not written in Java, accounting for 17.53\% of out dataset. We then computed the average of these indices to obtain the average MI for each malware. This number can be any number less than 171. In order to make the result easier to understand, we followed the same technique used by \textit{Visual Studio} \cite{b19}: any negative number is treated as 0, after which we rebase the range between 0 and 171 to be from 0 to 100. The maintainability index results are considered very low if the MI is between 0-10, moderately low if it ranges from 10-20; any MI above 20 is considered good enough.

Figure \ref{fig:samples_mi} shows the distribution of MI values grouped into 10-point intervals. Most samples have an MI between 40 and 60, with only 2 samples falling bellow 20, which is the recommended threshold: \textit{AndroidTrojanStarter} (Dropper, 2016) and \textit{GetFiles} (Spyware, 2021). The average MI is \textit{48.60}. There is one sample that falls in the last 10-point interval: \textit{FakeFacebook} (Trojan, 2013).

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			ybar,
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={No. Malware Samples},
			xlabel={Maintainability Index Values},
			symbolic x coords={0-10, 10-20, 20-30, 30-40, 40-50, 50-60, 60-70, 70-80, 80-90, 90-100},
			xtick=data,
			tick label style={font=\tiny},
			nodes near coords, 
			nodes near coords align={vertical}
			]
			\addplot table[x = range, y = count] {data/miagg.dat};
		\end{axis}
	\end{tikzpicture}
	\caption{Number of Samples by Maintainability Index}
	\label{fig:samples_mi}
\end{figure}

The annual distribution (Figure \ref{fig:mi_year}) shows a decreasing trend in maintainability of \textit{-0.2}, with a \textit{.0018} coefficient of determination. It is worth noting that, in this case, complexity is not related to maintainability, as both decrease each year rather than being inversely proportional.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
			anchor=north,legend columns=-1},
			ylabel={Maintainability Index},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 110,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = mi] {data/mi.dat};
			\addplot[thick, orange,domain=2012:2022]{-0.2041*x+460.5179};
			\addlegendentry{Maintainability Index}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{-0.2041}
				\cdot x
				\pgfmathprintnumber[print sign]{460.5179}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Maintainability Index by Year}
	\label{fig:mi_year}
\end{figure}

\subsubsection{\textbf{Density of comments}}
\label{subsection:comments}

Code documentation is of great importance in determining the maintainability of code. For this reason, it is relevant to check the ratio of comments compared to SLOCs. From Figure \ref{fig:comment_year} it can be seen that the tendency to include comments in the code is increasing. Specifically, the linear regression has a slope of \textit{0.34} and a coefficient of determination of \textit{.003}. The average comment ratio is \textit{14.81\%}.

\begin{figure}
	\begin{tikzpicture}
		\begin{axis}[
			/pgf/number format/1000 sep={},
			enlargelimits=0.15,
			legend style={at={(0.5,-0.15)},
				anchor=north,legend columns=-1},
			ylabel={Comment Ratio},
			xtick=data,
			x tick label style={rotate=90,anchor=east},
			ymin = 0, ymax = 100,
			legend columns=1,
			legend style={
				cells={anchor=west},
				font=\scriptsize,
				legend pos= north west
			},
			]
			\addplot[teal, only marks, mark=o] table[x = year, y = comments] {data/comments.dat};
			\addplot[thick, orange,domain=2012:2022]{0.3383*x-668.0969};
			\addlegendentry{Comment Ratio}
			\addlegendentry{
				Linear regression: $ y =
				\pgfmathprintnumber{0.3383}
				\cdot x
				\pgfmathprintnumber[print sign]{-668.0969}$
			}
		\end{axis}
	\end{tikzpicture}
	\caption{Comment Ratio by Year}
	\label{fig:comment_year}
\end{figure}

\section{Comparison with Other Malware}
\label{ch:comparison_malware}

In this section, we compare our results with those obtained by Calleja, Tapiador and Caballero in their work \cite{b5}. This will provide information on the differences present between general and Android-specific malware.

First of all, there is a big difference in the size of the dataset. Theirs contains 456 samples, being almost 5 times larger than ours, and spans from 1975 to 2016, which is an interval 4 times larger than that of our dataset.

In terms of size, Android samples tend to be larger: few of their samples manage to exceed the average number of files, SLOCs and number of programming languages obtained from Android malware. This can be explained by the prevalence of small samples in their dataset that only consist of 1 file and have few lines of code, but also because Android programs usually need a large number of files, as they must contain all view, logic and build files. The same is true for the values of effort, development time and team size: Android samples present higher values compared to non-Android malware.

But this is reversed when complexity and maintainability are compared. Android malware shows extremely low values in terms of complexity compared to non-specific malware. Maintainability and feedback ratios are also poor in comparison. In essence, one could argue that Android malware has larger sizes and development costs, but is less complex and has lower quality compared to other types of malware.

\section{Discussion}
\label{ch:discussion}

Bellow, we discuss some aspects of the nature of our approach, mainly the \nameref{subsection:limitations} of our methodologies and results, and pose some \nameref{subsection:questions} for future work.

\subsection{Limitations}
\label{subsection:limitations}

One of the potential shortcoming of this work is the low number of samples, as it renders the analysis less reliable and representative. However, obtaining source code is an arduous task, even more so in the case of malware samples, especially Android ones. Considering this fact, we believe that our dataset is representative enough to at least ensure that the analysis can extract some genuine results.

Our dataset possibly suffers from a collection bias, as all samples have been gathered from public GitHub repositories, which do not necessarily represent malware used in the wild. Still, we have a decent number of leaked malware samples that have been engineered by malware developers and openly deployed. Nevertheless, the richness of our dataset would increase considerably if more samples were collected from specialized underground forums and other means.

We considered the number of functions as a metric of code size, but it would also have been valuable to consider \textit{function point estimates} \cite{b20}, which measure external inputs and outputs, files used, user interactions and external interfaces to capture the overall functionality of the software.

\textit{UCC-J} \cite{b15} has proven to be a limited tool, especially in the maintainability index calculations, considering that Kotlin, which along with Java is the most common language for developing Android applications, is not supported.

Lastly, we consider that it is still too early to draw meaningful conclusions, as only a decade has passed since Android was released. Another decade would provide more data that would allow researches to draw more accurate results. 

\subsection{Open questions}
\label{subsection:questions}

There are some analyses that were planned, but were not carried out, as we did not have enough time to perform them. One of them is to analyze whether certain malware types have increased in popularity over the years. 

Another analysis we left behind is to distribute languages by malware type, to see which programming languages are most used by each type of malware. In this paper we already mentioned that RATs and Spyware tend to have server-side languages apart from client code, but it would be interesting to make that comparison with all malware types.

It would also be insightful to contrast the metrics of malware software with those of regular software, to see if there are major differences between the costs, maintainability and complexity of regular development teams compared to malware-focused ones.

We also wanted to compile each sample to see possible errors, as well as its behavior in a sandbox environment and perform a dynamic analysis.

For each application, we also wanted to know how many components, services, etc. the sample has and what uses each of them serves.

Finally, code reuse and clone detection was one of the most important analyses we had to leave out. It would have been extremely revealing to measure plagiarism, code sharing, auto-generated code and code reuse among the malware samples.

\section{Related Work}
\label{ch:related_work}

Calleja, Tapiador and Caballero \cite{b4}, \cite{b5} followed the same approach as this paper, as we have based this thesis on their works. They analyzed a large number of malware source code samples from different operating systems and from different time periods for code size, code quality, development costs and code reuse. They found that malware production is progressively becoming an industry, as code size, effort and complexity experienced an exponential increase over the years.

Mercaldo, Di Sorbo, Visaggio, Cimitile, and Martinelli \cite{b21} demonstrated that malware writers devote effort and skill to improve the quality of their code to produce high quality malware, similar to the evolution observed in goodware applications.

Yajin Zhou and Xuxian Jiang \cite{b22} collected 1200 Android malware samples covering major families and arranged them according to various aspects, after which they revealed the rapid evolution of malware to evade anti-virus software, revealing that some antivirus software are not up to the malware's efforts. Tam, Feizollah, Anuar, Salleh and Cavallaro \cite{b23} similarly analyzed detection techniques against Android malware, seeking to demonstrate the evolution of Android malware, and also discuss related malware statistics.

\section{Conclusion}
\label{ch:conclusion}

In this thesis, we have conducted a study on the evolution of Android malware source code over its entire lifetime, which for now spans a period of 10 years. We have collected and analyzed 97 samples, which is the largest dataset of Android malware source code to our knowledge. We have quantified the code size and estimated its cost and quality using well-known software metrics. The results extracted from this work indicate an increase in size and cost, but a small decrease in complexity and maintainability. Therefore, we conclude that the results are not conclusive enough to support the claim of a developing malware production industry.

A CSV file with all the information we have collected and extracted from the malware samples during this research can be obtained from the following GitHub repository \cite{b24}.

\begin{thebibliography}{00}
\bibitem{b1} A. Cranz, “There are over 3 billion active Android devices,” The Verge, 18-May-2021. [Online]. Available: \href{https://www.theverge.com/2021/5/18/22440813/android-devices-active-number-smartphones-google-2021}{https://www.theverge.com/2021/5/18/22440813/android-devices-active-number-smartphones-google-2021}. [Accessed: 19-Sep-2022]. 
\bibitem{b2} “Malware Statistics \& Trends Report: AV-TEST,” AV. [Online]. Available: href{https://www.av-test.org/en/statistics/malware/}{https://www.av-test.org/en/statistics/malware/}. [Accessed: 19-Sep-2022]. 
\bibitem{b3} A. Kujawa, J. Segura, T. Reed, N. Collier, J. Taggart, H. Jazi, A. Brading, M. Stocklev, D. Ruiz, J. Umawing, C. Boyd, P. Arntz, “2021, State of Malware Report,” Malware Bytes. [Online]. Available: \href{https://www.malwarebytes.com/resources/files/2021/02/mwb_stateofmalwarereport2021.pdf}{https://www.malwarebytes.com/resources/files/2021/02/mwb\_stateofmalwarereport2021.pdf}. [Accessed: 19-Sep-2022]. 
\bibitem{b4} A. Calleja, J. Tapiador, and J. Caballero, “A Look into 30 Years of Malware Development from a Software Metrics Perspective,” in Proceedings of the 19th International Symposium on Research in Attacks, Intrusions and Defenses, pp. 325-345, Evry, France, September 2016, doi: 10.1007/978-3-319-45719-2\_15.
\bibitem{b5} A. Calleja, J. Tapiador and J. Caballero, "The MalSource Dataset: Quantifying Complexity and Code Reuse in Malware Development," in IEEE Transactions on Information Forensics and Security, vol. 14, no. 12, pp. 3175-3190, Dec. 2019, doi: 10.1109/TIFS.2018.2885512.
\bibitem{b6} “Where the world builds software,” GitHub. [Online]. Available: \href{https://github.com/}{https://github.com/}. [Accessed: 16-Sep-2022]. 
\bibitem{b7} Vxunderground, “MalwareSourceCode: Collection of malware source code for a variety of platforms in an array of different programming languages.,” GitHub. [Online]. Available: \href{https://github.com/vxunderground/MalwareSourceCode}{https://github.com/vxunderground/MalwareSourceCode}. [Accessed: 16-Sep-2022]. 
\bibitem{b8} “vx-underground,” vx-underground. [Online]. Available: \href{https://www.vx-underground.org/}{https://www.vx-underground.org/}. [Accessed: 16-Sep-2022]. 
\bibitem{b9} “A new virus naming convention (1991),” CARO. [Online]. Available: \href{http://www.caro.org/articles/naming.html}{http://www.caro.org/articles/naming.html}. [Accessed: 16-Sep-2022]. 
\bibitem{b10} d-Raco, “android-malware-source-code-samples: Android malware source code dataset collected from public resources.,” GitHub. [Online]. Available: \href{https://github.com/d-Raco/android-malware-source-code-samples}{https://github.com/d-Raco/android-malware-source-code-samples}. [Accessed: 16-Sep-2022]. 
\bibitem{b11} d-Raco, “android-malware-capabilities-analyzer:  A tool for analyzing Android malware source code capabilities.,” GitHub. [Online]. Available: \href{https://github.com/d-Raco/android-malware-capabilities-analyzer}{https://github.com/d-Raco/android-malware-capabilities-analyzer}. [Accessed: 16-Sep-2022]. 
\bibitem{b12} “Manifest.permission: android developers,” Android Developers. [Online]. Available: \href{https://developer.android.com/reference/android/Manifest.permission}{https://developer.android.com/reference/android/Manifest.permission}. [Accessed: 16-Sep-2022]. 
\bibitem{b13} “VirusTotal - Home,” VirusTotal. [Online]. Available: \href{https://www.virustotal.com/gui/home/upload}{https://www.virustotal.com/gui/home/upload}. [Accessed: 17-Sep-2022]. 
\bibitem{b14} AlDanial, “Cloc: Cloc Counts Blank Lines, comment lines, and physical lines of source code in many programming languages.,” GitHub. [Online]. Available: \href{https://github.com/AlDanial/cloc}{https://github.com/AlDanial/cloc}. [Accessed: 18-Sep-2022]. 
\bibitem{b15} “CSSE tools / Unified Code Count Java - UCC-J / UCC-java 2018.05,” GitLab. [Online]. Available: \href{https://cssedr.usc.edu:4443/csse-tools/ucc-j/UCC-Java-2018.05}{https://cssedr.usc.edu:4443/csse-tools/ucc-j/UCC-Java-2018.05}. [Accessed: 18-Sep-2022]. 
\bibitem{b16} “Boehm Center for Systems and Software Engineering,” CSSE. [Online]. Available: https://csse.usc.edu/. [Accessed: 18-Sep-2022]. 
\bibitem{b17} B. W. Boehm, "Software Engineering Economics," in IEEE Transactions on Software Engineering, vol. SE-10, no. 1, pp. 4-21, Jan. 1984, doi: 10.1109/TSE.1984.5010193. 
\bibitem{b18} T. J. McCabe, "A Complexity Measure," in IEEE Transactions on Software Engineering, vol. SE-2, no. 4, pp. 308-320, Dec. 1976, doi: 10.1109/TSE.1976.233837.
\bibitem{b19} “Code metrics - maintainability index range and meaning - Visual Studio (Windows),” Visual Studio (Windows) | Microsoft Learn. [Online]. Available: \href{https://learn.microsoft.com/en-us/visualstudio/code-quality/code-metrics-maintainability-index-range-and-meaning?view=vs-2022}{https://learn.microsoft.com/en-us/visualstudio/code-quality/code-metrics-maintainability-index-range-and-meaning?view=vs-2022}. [Accessed: 19-Sep-2022]. 
\bibitem{b20} A. J. Albrecht, “Measuring Application Development Productivity,” in
IBM Application Development Symp., I. B. M. Press, Ed., Oct. 1979.
\bibitem{b21} F. Mercaldo, A. Di Sorbo, C. A. Visaggio, A. Cimitile, and F. Martinelli, “An exploratory study on the evolution of Android Malware Quality,” Journal of Software: Evolution and Process, vol. 30, no. 11, 2018. 
\bibitem{b22} Y. Zhou and X. Jiang, "Dissecting Android Malware: Characterization and Evolution," 2012 IEEE Symposium on Security and Privacy, 2012, pp. 95-109, doi: 10.1109/SP.2012.16.
\bibitem{b23} K. Tam, A. Feizollah, N. B. Anuar, R. Salleh, and L. Cavallaro, “The evolution of Android malware and Android Analysis Techniques,” ACM Computing Surveys, vol. 49, no. 4, pp. 1–41, 2017. 
\bibitem{b24} d-Raco, “android-malware-source-code-analysis: Analysis of Android malware families using available source code.,” GitHub. [Online]. Available: \href{https://github.com/d-Raco/android-malware-source-code-analysis}{https://github.com/d-Raco/android-malware-source-code-analysis}. [Accessed: 16-Sep-2022]. 
\end{thebibliography}

\end{document}
